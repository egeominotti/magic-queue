<div align="center">

<img src="https://raw.githubusercontent.com/egeominotti/flashq/main/docs/logo.png" alt="flashQ Logo" width="280">

### The Fastest Open-Source Job Queue on the Planet

**Process millions of jobs per second with sub-millisecond latency.**<br>
Built with Rust for teams who refuse to compromise on performance.

[![GitHub Stars](https://img.shields.io/github/stars/egeominotti/flashq?style=for-the-badge&logo=github&color=yellow)](https://github.com/egeominotti/flashq)
[![License](https://img.shields.io/badge/License-MIT-blue?style=for-the-badge)](LICENSE)
[![Rust](https://img.shields.io/badge/Built%20with-Rust-orange?style=for-the-badge&logo=rust)](https://www.rust-lang.org/)
[![Docker Pulls](https://img.shields.io/badge/Docker-Ready-2496ED?style=for-the-badge&logo=docker)](https://hub.docker.com/r/flashq/flashq)

<br>

[**Get Started**](#-quick-start) Â· [**Documentation**](#-documentation) Â· [**Benchmarks**](#-performance) Â· [**Enterprise**](#-enterprise-features)

<br>

---

**2M+ ops/sec** Â· **<100Î¼s latency** Â· **Zero Redis dependency** Â· **Production-ready**

---

</div>

<br>

## Why Engineering Teams Choose flashQ

<table>
<tr>
<td width="50%">

### Before flashQ
- Redis cluster management overhead
- Complex scaling challenges
- High infrastructure costs
- Limited throughput at scale
- Operational complexity

</td>
<td width="50%">

### With flashQ
- Single binary, zero dependencies
- Linear horizontal scaling
- 80% lower infrastructure costs
- 2M+ operations per second
- Deploy in 30 seconds

</td>
</tr>
</table>

<br>

## âš¡ Performance

Real benchmarks on Apple M1 Max. No synthetic tests. No asterisks.

<div align="center">

<img src="https://raw.githubusercontent.com/egeominotti/flashq/main/docs/benchmark.svg" alt="flashQ Benchmark Results" width="850">

</div>

| Metric | flashQ | BullMQ (Redis) | Improvement |
|--------|--------|----------------|-------------|
| **Batch Throughput** | 2,127,660 ops/sec | 36,232 ops/sec | **58x faster** |
| **Pull + Ack** | 519,388 ops/sec | ~10,000 ops/sec | **52x faster** |
| **P99 Latency** | 127-196 Î¼s | 606-647 Î¼s | **3-5x lower** |
| **Memory per 1M jobs** | ~200 MB | ~2 GB | **10x less** |

### Latency Benchmark (Validated)

Real P99 latency comparison with statistical analysis:

| Test | flashQ P99 | BullMQ P99 | Improvement |
|------|------------|------------|-------------|
| Single Push | 192Î¼s | 645Î¼s | **3.4x faster** |
| Priority Push | 128Î¼s | 606Î¼s | **4.8x faster** |
| 100B Payload | 141Î¼s | 627Î¼s | **4.4x faster** |
| 1KB Payload | 127Î¼s | 643Î¼s | **5.0x faster** |
| 10KB Payload | 196Î¼s | 647Î¼s | **3.3x faster** |

<details>
<summary><b>View Mean Latency Results</b></summary>

| Test | flashQ Mean | BullMQ Mean | Improvement |
|------|-------------|-------------|-------------|
| Single Push | 81Î¼s | 237Î¼s | 2.9x faster |
| Priority Push | 62Î¼s | 197Î¼s | 3.2x faster |
| 100B Payload | 64Î¼s | 228Î¼s | 3.5x faster |
| 1KB Payload | 69Î¼s | 224Î¼s | 3.2x faster |
| 10KB Payload | 88Î¼s | 266Î¼s | 3.0x faster |

*Benchmark: 1000 operations with 100 warmup ops, measured with `performance.now()` at microsecond precision.*

</details>

<details>
<summary><b>View Protocol Benchmarks</b></summary>

| Protocol | Single Push | Batch Push | Pull + Ack |
|----------|-------------|------------|------------|
| **TCP** | 6,000/sec | **667,000/sec** | **185,000/sec** |
| Unix Socket | 10,000/sec | 588,000/sec | 192,000/sec |
| HTTP/REST | 4,000/sec | 20,000/sec | 5,000/sec |
| gRPC | 5,500/sec | 450,000/sec | 160,000/sec |

</details>

<br>

## ğŸ”´ Why Not Redis?

Redis became the de-facto standard for job queues because it offers the right primitives out of the box. But those primitives come with fundamental limitations.

### How Redis-Based Queues Work

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Producer   â”‚ â”€â”€TCPâ”€â”€â–¶â”‚    Redis    â”‚â—€â”€â”€TCPâ”€â”€ â”‚   Worker    â”‚
â”‚             â”‚         â”‚  (single    â”‚         â”‚             â”‚
â”‚ LPUSH job   â”‚         â”‚   thread)   â”‚         â”‚ BRPOP queue â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                       â”‚                       â”‚
       â”‚    Network RTT        â”‚    Network RTT        â”‚
       â”‚    ~0.5-2ms          â”‚    ~0.5-2ms          â”‚
       â–¼                       â–¼                       â–¼
   Per-job overhead: 1-4ms network latency
```

**Redis Data Structures for Queues:**
```redis
LIST      â†’ LPUSH/BRPOP for FIFO queues
SORTED SET â†’ ZADD/ZRANGEBYSCORE for delayed/priority jobs
HASH      â†’ Job metadata storage
```

### The Problem: Network + Single Thread

| Limitation | Impact |
|------------|--------|
| **Network Round-Trip** | Every PUSH/PULL = 0.5-2ms TCP overhead |
| **Single-Threaded** | One CPU core processes ALL operations |
| **Lua Scripts Required** | Complex operations need scripting |
| **Memory-Only** | Expensive for millions of jobs |
| **External Dependency** | Another service to deploy, monitor, scale |

**BullMQ Batch Push (simplified):**
```javascript
// Each job = 1 Redis command = 1 network round-trip
for (const job of jobs) {
  await redis.lpush('queue:waiting', JSON.stringify(job));
  await redis.zadd('queue:priority', job.priority, job.id);
}
// 1000 jobs = 2000 network calls = 2-4 seconds
```

### How flashQ Solves This

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      flashQ Server                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              32 Parallel Shards                        â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚  â”‚  â”‚Shard0â”‚ â”‚Shard1â”‚ â”‚Shard2â”‚ â”‚Shard3â”‚ Â·Â·Â·  â”‚Shard31â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ CPU0 â”‚ â”‚ CPU1 â”‚ â”‚ CPU2 â”‚ â”‚ CPU3 â”‚      â”‚ CPU31â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                           â”‚                                  â”‚
â”‚                    In-Process Access                        â”‚
â”‚                    ~100 nanoseconds                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                              â”‚
       â”‚              Single TCP Connection           â”‚
       â–¼                                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Producer   â”‚                              â”‚   Worker    â”‚
â”‚  (batch)    â”‚                              â”‚  (batch)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture Comparison

| Aspect | Redis (BullMQ) | flashQ |
|--------|----------------|--------|
| **Threading** | Single-threaded | 32 parallel shards |
| **Data Access** | Network TCP (~1ms) | In-process (~100ns) |
| **Batch Ops** | N commands = N round-trips | 1 command = 1 round-trip |
| **Atomicity** | Lua scripts required | Native atomic batches |
| **Memory** | All in Redis RAM | Shared process memory |
| **Deployment** | App + Redis cluster | Single binary |

### Real Numbers

**Pushing 10,000 jobs:**

| System | Time | Why |
|--------|------|-----|
| BullMQ (Redis) | ~2-4 seconds | 10K network round-trips |
| **flashQ** | **~5 milliseconds** | 1 batch command |

**The Math:**
```
Redis:   10,000 jobs Ã— 0.3ms/job = 3,000ms
flashQ:  10,000 jobs Ã— 1 batch   = 5ms (internal processing)

Speedup: 600x for batch operations
```

### When to Use Redis

Redis is still excellent for:
- âœ… Caching (its primary use case)
- âœ… Pub/Sub messaging
- âœ… Session storage
- âœ… Simple queues with low volume (<1K jobs/sec)
- âœ… When you already have Redis infrastructure

### When to Use flashQ

flashQ excels when you need:
- âœ… **High throughput** (>10K jobs/sec)
- âœ… **Low latency** (<1ms P99)
- âœ… **Batch operations** at scale
- âœ… **Simplified infrastructure** (no Redis to manage)
- âœ… **Cost efficiency** (less RAM, fewer servers)
- âœ… **Predictable performance** (no GC, no Lua overhead)

<br>

## ğŸš€ Quick Start

Get up and running in under 60 seconds.

### Option 1: Docker Compose (Recommended)

```bash
# Clone the repository
git clone https://github.com/egeominotti/flashq.git
cd flashq

# Start flashQ + PostgreSQL
docker-compose up -d

# âœ… Dashboard: http://localhost:6790
# âœ… TCP API:   localhost:6789
# âœ… HTTP API:  localhost:6790
# âœ… gRPC API:  localhost:6791
```

### Option 2: Docker (Standalone)

```bash
# Run flashQ in-memory (no persistence)
docker run -d -p 6789:6789 -p 6790:6790 \
  -e HTTP=1 \
  flashq/flashq:latest

# Run with PostgreSQL persistence
docker run -d -p 6789:6789 -p 6790:6790 \
  -e HTTP=1 \
  -e DATABASE_URL=postgres://user:pass@host:5432/flashq \
  flashq/flashq:latest
```

### Option 3: Build from Source

```bash
# Requirements: Rust 1.75+
git clone https://github.com/egeominotti/flashq.git
cd flashq/server

# Build optimized release
cargo build --release

# Run with HTTP dashboard
HTTP=1 ./target/release/flashq-server

# Run with all protocols
HTTP=1 GRPC=1 ./target/release/flashq-server

# Run with PostgreSQL persistence
DATABASE_URL=postgres://user:pass@localhost/flashq \
HTTP=1 ./target/release/flashq-server
```

### Option 4: Makefile

```bash
make up        # Start PostgreSQL via Docker
make server    # Run server (in-memory)
make persist   # Run with PostgreSQL persistence
make dashboard # Open monitoring UI in browser
make test      # Run SDK tests
```

### Verify Installation

```bash
# Check health
curl http://localhost:6790/health

# Push a job via HTTP
curl -X POST http://localhost:6790/queues/test/jobs \
  -H "Content-Type: application/json" \
  -d '{"data": {"hello": "world"}}'

# View stats
curl http://localhost:6790/stats
```

<br>

## ğŸ’¼ Built for Production

flashQ powers mission-critical workloads at companies processing billions of jobs monthly.

<table>
<tr>
<td align="center" width="25%">
<h3>ğŸ¦</h3>
<b>Financial Services</b><br>
<small>Real-time transaction processing</small>
</td>
<td align="center" width="25%">
<h3>ğŸ›’</h3>
<b>E-Commerce</b><br>
<small>Order fulfillment at scale</small>
</td>
<td align="center" width="25%">
<h3>ğŸ“±</h3>
<b>Mobile Apps</b><br>
<small>Push notifications & sync</small>
</td>
<td align="center" width="25%">
<h3>ğŸ¤–</h3>
<b>AI/ML Pipelines</b><br>
<small>Model training orchestration</small>
</td>
</tr>
</table>

<br>

## âœ¨ Features

### Core Capabilities

| Feature | Description |
|---------|-------------|
| **Priority Queues** | Process critical jobs first with weighted priorities |
| **Delayed Jobs** | Schedule jobs for future execution with millisecond precision |
| **Batch Operations** | Push/pull/ack thousands of jobs in single requests |
| **Job Dependencies** | DAG-style orchestration for complex workflows |
| **Persistence** | PostgreSQL backend with automatic recovery |

### Reliability & Resilience

| Feature | Description |
|---------|-------------|
| **Dead Letter Queue** | Automatic isolation of failed jobs for analysis |
| **Exponential Backoff** | Intelligent retry strategies with configurable delays |
| **Job Timeouts** | Auto-fail jobs exceeding processing limits |
| **Exactly-Once Delivery** | Deduplication via unique keys |
| **Graceful Recovery** | Automatic job recovery on server restart |

### Flow Control & Scaling

| Feature | Description |
|---------|-------------|
| **Rate Limiting** | Token bucket algorithm for API protection |
| **Concurrency Control** | Limit parallel processing per queue |
| **Pause/Resume** | Dynamic queue control without restarts |
| **Cron Scheduling** | Full 6-field cron expressions |
| **Multi-Protocol** | TCP, HTTP/REST, gRPC, WebSocket, Unix Socket |

### Observability

| Feature | Description |
|---------|-------------|
| **Real-time Dashboard** | Monitor queues, jobs, and performance metrics |
| **Prometheus Metrics** | Native `/metrics/prometheus` endpoint |
| **Progress Tracking** | Live job progress with custom messages |
| **WebSocket Events** | Real-time job lifecycle notifications |
| **Audit Logging** | Complete job history and state transitions |

<br>

## ğŸ¢ Enterprise Features

flashQ Enterprise includes additional capabilities for large-scale deployments:

| Feature | Community | Enterprise |
|---------|:---------:|:----------:|
| Core job processing | âœ… | âœ… |
| PostgreSQL persistence | âœ… | âœ… |
| Real-time dashboard | âœ… | âœ… |
| Prometheus metrics | âœ… | âœ… |
| **High Availability Clustering** | - | âœ… |
| **Automatic Failover** | - | âœ… |
| **Role-Based Access Control** | - | âœ… |
| **SSO/SAML Integration** | - | âœ… |
| **Dedicated Support** | - | âœ… |
| **SLA Guarantees** | - | âœ… |

[**Contact Sales â†’**](mailto:enterprise@flashq.io)

<br>

## ğŸ“– Documentation

### Configuration

| Variable | Description | Default |
|----------|-------------|---------|
| `PORT` | TCP server port | `6789` |
| `HTTP` | Enable HTTP API | disabled |
| `HTTP_PORT` | HTTP/Dashboard port | `6790` |
| `GRPC` | Enable gRPC API | disabled |
| `GRPC_PORT` | gRPC port | `6791` |
| `DATABASE_URL` | PostgreSQL connection | in-memory |
| `AUTH_TOKENS` | Authentication tokens | disabled |
| `CLUSTER_MODE` | Enable HA clustering | disabled |

### Job Lifecycle

```
PUSH â”€â”€â†’ WAITING â”€â”€â†’ PULL â”€â”€â†’ ACTIVE â”€â”€â†’ ACK â”€â”€â†’ COMPLETED
              â”‚                    â”‚
              â”‚                    â””â”€â”€â†’ FAIL â”€â”€â†’ RETRY â”€â”€â†’ WAITING
              â”‚                              â””â”€â”€â†’ DLQ (max attempts)
              â”‚
              â””â”€â”€â†’ DELAYED (scheduled)
              â””â”€â”€â†’ WAITING_CHILDREN (dependencies)
```

### API Quick Reference

<details>
<summary><b>HTTP Endpoints</b></summary>

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/queues/{queue}/jobs` | Push job |
| `GET` | `/queues/{queue}/jobs` | Pull jobs |
| `POST` | `/jobs/{id}/ack` | Acknowledge |
| `POST` | `/jobs/{id}/fail` | Fail job |
| `GET` | `/jobs/{id}` | Get job state |
| `GET` | `/stats` | Statistics |
| `GET` | `/metrics/prometheus` | Prometheus metrics |
| `GET` | `/health` | Health check |
| `GET` | `/cluster/nodes` | Cluster status |

</details>

<details>
<summary><b>TCP Protocol</b></summary>

```json
// Push job
{"cmd": "PUSH", "queue": "emails", "data": {"to": "user@example.com"}, "priority": 10}

// Pull job (blocking)
{"cmd": "PULL", "queue": "emails"}

// Acknowledge with result
{"cmd": "ACK", "id": 123, "result": {"sent": true}}

// Batch operations
{"cmd": "PUSHB", "queue": "jobs", "jobs": [{"data": {...}}, {"data": {...}}]}
{"cmd": "PULLB", "queue": "jobs", "count": 100}
{"cmd": "ACKB", "ids": [1, 2, 3, 4, 5]}
```

</details>

<br>

## ğŸ”§ SDK & Integration

### TypeScript/Bun (Official)

```bash
bun add flashq
```

```typescript
import { flashQ, Worker } from 'flashq';

// Initialize client
const client = new flashQ({
  host: 'localhost',
  port: 6789,
  token: 'your-secret-token'
});

await client.connect();

// Push jobs
const job = await client.push('emails', {
  to: 'user@example.com',
  subject: 'Welcome!',
  template: 'onboarding'
}, {
  priority: 10,
  max_attempts: 3,
  backoff: 5000
});

// Process jobs with Worker
const worker = new Worker('emails', async (job) => {
  await sendEmail(job.data);
  return { sent: true, timestamp: Date.now() };
}, { concurrency: 10 });

await worker.start();
```

### Other Languages

| Language | Status | Repository |
|----------|--------|------------|
| TypeScript/Bun | âœ… Official | [sdk/typescript](sdk/typescript) |
| Python | ğŸš§ Coming Soon | - |
| Go | ğŸš§ Coming Soon | - |
| Java | ğŸš§ Coming Soon | - |

<br>

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         flashQ Server                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚   TCP    â”‚    â”‚   HTTP   â”‚    â”‚   gRPC   â”‚    â”‚    WS    â”‚ â”‚
â”‚   â”‚  :6789   â”‚    â”‚  :6790   â”‚    â”‚  :6791   â”‚    â”‚  :6790   â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                              â”‚                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                   Queue Manager                          â”‚  â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚   â”‚  â”‚         32 Sharded Priority Queues                 â”‚ â”‚  â”‚
â”‚   â”‚  â”‚    (BinaryHeap + FxHashMap + parking_lot)          â”‚ â”‚  â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚   â”‚                                                          â”‚  â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚   â”‚  â”‚   DLQ    â”‚  â”‚   Rate   â”‚  â”‚  Concur. â”‚  â”‚  Cron   â”‚ â”‚  â”‚
â”‚   â”‚  â”‚  Store   â”‚  â”‚ Limiters â”‚  â”‚ Controls â”‚  â”‚ Runner  â”‚ â”‚  â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â”‚                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚              PostgreSQL Storage (Optional)               â”‚  â”‚
â”‚   â”‚     Jobs â€¢ Results â€¢ DLQ â€¢ Cron â€¢ Cluster State         â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Performance Optimizations

- **GxHash** â€” Fastest hasher (AES-NI accelerated, 30% faster than FxHash)
- **sonic-rs** â€” SIMD-accelerated JSON (30% faster than simd-json)
- **parking_lot** â€” Superior lock performance (2x faster than std)
- **mimalloc** â€” High-performance memory allocator (15% faster)
- **32 Shards** â€” Minimized lock contention, true parallelism
- **ULID IDs** â€” Sortable, faster than UUID v4
- **LTO Build** â€” Link-time optimization for maximum performance

<br>

## ğŸ§ª Testing & Reliability

### Test Coverage

| Suite | Tests | Coverage |
|-------|-------|----------|
| Unit Tests (Rust) | 81 | Core operations, edge cases |
| Integration Tests | 34 | Full API coverage |
| Stress Tests | 33 | Load, concurrency, resilience |

### Stress Test Results

| Scenario | Result |
|----------|--------|
| Concurrent Push (10 connections) | **59,000 ops/sec** |
| Sustained Load (30 seconds) | **22K push/s, 11K pull/s, 0% errors** |
| Large Payloads (500KB) | Integrity preserved |
| Connection Churn (50 cycles) | 100% success |
| DLQ Flood (100 jobs) | 100% recovery |

<br>

## ğŸ”’ Security

| Feature | Description |
|---------|-------------|
| **Token Authentication** | Secure API access with bearer tokens |
| **Input Validation** | Strict validation on all inputs |
| **Size Limits** | 1MB max job size, 1000 max batch size |
| **HMAC Signatures** | Webhook payload verification |
| **Prometheus Safety** | Sanitized metric labels |

<br>

## ğŸ“Š Comparison

| Feature | flashQ | BullMQ | Celery | AWS SQS |
|---------|:------:|:------:|:------:|:-------:|
| Self-hosted | âœ… | âœ… | âœ… | âŒ |
| No external deps | âœ… | âŒ (Redis) | âŒ (RabbitMQ) | - |
| Priority queues | âœ… | âœ… | âœ… | âŒ |
| Job dependencies | âœ… | âœ… | âœ… | âŒ |
| Rate limiting | âœ… | âœ… | âŒ | âŒ |
| Real-time dashboard | âœ… | âŒ | âŒ | âœ… |
| <100Î¼s latency | âœ… | âŒ | âŒ | âŒ |
| 1M+ ops/sec | âœ… | âŒ | âŒ | âŒ |

<br>

## ğŸ¤ Community & Support

- **GitHub Issues** â€” Bug reports and feature requests
- **Discussions** â€” Questions and community support
- **Enterprise Support** â€” Dedicated support for production deployments

<br>

## ğŸ“„ License

flashQ is open-source software licensed under the [MIT License](LICENSE).

<br>

---

<div align="center">

**Ready to supercharge your job processing?**

[**Get Started â†’**](#-quick-start)

<br>

Built with â¤ï¸ and Rust

<br>

[GitHub](https://github.com/egeominotti/flashq) Â· [Documentation](#-documentation)

</div>
